# Plan de Diseño Profesional para "ComputeNodes" (Blender 5.0+ Addon)

## **Introducción y Estado Actual del MVP**

El addon **ComputeNodes** es un prototipo de sistema de **nodos de cómputo GPU** para Blender 4.x/5.x, enfocado en procesar imágenes 2D mediante _compute shaders_. Emplea un **árbol de nodos personalizado** (ComputeNodeTree) con nodos que generan código GLSL y ejecutan en GPU. Actualmente, el MVP soporta operaciones matemáticas básicas, texturas procedurales (ruido, voronoi, etc.), manipulación de vectores y color, bucles sencillos y E/S de imágenes. La arquitectura interna se basa en una representación intermedia (_IR_) SSA con opcodes definidos, un planificador de _passes_ de cómputo y un ejecutor que usa la API GPU de Blender.

**Limitaciones actuales:** El diseño MVP presenta varias decisiones simplificadas que debemos revisar y mejorar. Por ejemplo, **solo maneja imágenes 2D en una resolución global fija** (tomada del primer nodo de salida o de la resolución de render por defecto)[\[1\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/operators.py#L46-L55)[\[2\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/operators.py#L56-L64), no contempla directamente datos 1D/3D ni múltiples resoluciones en un mismo gráfico. La implementación de bucles está **hardcodeada** para un caso muy específico (un acumulador escalar)[\[3\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/graph_extract_old.py#L801-L810), y algunos nodos/recursos se manejan con suposiciones frágiles (e.g. el nodo Output asume un _target_ de imagen conectado, pero su interfaz UI solo expone un socket de color). Además, ciertas partes del IR/código están incompletas o son inconsistentes (p. ej. manejo de multi-salida, nodos de escritura de imagen, etc.), lo que ofrece oportunidades claras de refactorización.

A continuación se presenta un plan de diseño detallado que eleva este MVP hacia un nivel comparable con Houdini **Copernicus** (el nuevo contexto de composición de Houdini 20.5/21), abordando soporte de **kernels 1D/2D/3D**, bucles avanzados, nuevos tipos de datos (partículas, volúmenes, etc.), nodos personalizables por el usuario, mejoras de arquitectura IR, manejo robusto de dominios/resoluciones, y una UX consistente con Blender. Cada recomendación se justifica técnicamente, señalando riesgos y consideraciones de implementación.

## **Soporte para Kernels 1D, 2D y 3D (evaluación de SSBO en Blender 2025)**

Actualmente ComputeNodes asume kernels 2D (imágenes bidimensionales) y lanza el _compute shader_ en un dominio 2D fijo (anchura/altura). Para alcanzar un nivel "Copernicus" debemos **extender el soporte a datos 1D y 3D**, lo que incluye imágenes 1D (líneas de píxeles), volúmenes 3D, o buffers arbitrarios. Proponemos lo siguiente:

- **Generalización de recursos de imagen:** Rediseñar la clase de recurso ImageDesc (o crear un ResourceDesc genérico) para incluir el **tipo de dimensionalidad** (1D, 2D, 3D). Esto permitirá que el IR diferencie entre, por ejemplo, image1D, image2D e image3D. En la generación de código GLSL, usaremos la dimensionalidad apropiada al declarar y acceder a la imagen. Por ejemplo, una imagen 3D se declarará como image3D y se indexará con coordenadas (x,y,z), mientras que una 1D usaría image1D con índice x. Actualmente el código asume image2D uniformemente[\[4\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/codegen/glsl.py#L88-L96), por lo que habría que hacerlo dependiente del tipo de recurso.
- **Ajuste del lanzamiento (_dispatch_) del cómputo:** Con kernels 1D/3D, el tamaño de grupo de cómputo y el número de grupos lanzados cambiarán. Blender permite invocar gpu.compute.dispatch(shader, nx, ny, nz) con grupos en X, Y y Z[\[5\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/tests/test_runtime.py#L140-L148)[\[6\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/tests/test_runtime.py#L150-L156). Para una imagen 1D, definiríamos ny=nz=1 y nx = ceil(longitud/local_size_x). Para una 3D, calcularemos los grupos en X, Y, Z según la profundidad. Es importante coordinar esto con la directiva layout(local_size_x, local_size_y, local_size_z) en el shader. Por ejemplo, podríamos usar local_size = (8,8,1) para 2D y (4,4,4) para 3D, según convenga. El _planner_ deberá asignar a cada **ComputePass** una dimensionalidad de ejecución (derivada de los recursos que procesa). Inicialmente podemos restringir: _todas_ las ops de un pass comparten la misma dimensionalidad (p.ej., si un pass escribe un volumen 3D, ejecutamos en 3D). Si se mezcla 2D y 3D en un mismo gráfico, el planificador debe segmentar en pases separados cuando cambie la dimensionalidad (extensión de la lógica de hazard actual). En el código ya se prevé dividir pases si cambian "requerimientos de dispatch"[\[7\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/planner/scheduler.py#L20-L28), funcionalidad que implementaremos para casos 1D/3D.
- **Buffers y SSBOs:** Para datos 1D no-imagen (por ejemplo, listas de partículas, curvas de valores, etc.), lo ideal es usar **SSBOs (Shader Storage Buffer Objects)** por su flexibilidad para almacenar arrays arbitrarios en shaders. Sin embargo, debemos verificar la viabilidad en Blender 5.0. A diciembre de 2025, la API gpu de Blender sigue madurando; en discusiones de desarrolladores se indica el deseo de exponer SSBOs pero aún no es claro si hay soporte completo[\[8\]](https://devtalk.blender.org/t/suggestions-feedback-on-the-extensions-for-the-gpu-module/17706/79?u=fclem#:~:text=Suggestions%20%2F%20feedback%20on%20the,sake%2C%20maybe%20something%20like). En ausencia de una envoltura directa de SSBO en Python, proponemos dos estrategias:
- **Emular buffers con texturas 1D:** Utilizar una textura 1D (por ejemplo RGBA32F) como contenedor de datos arbitrarios. Cada "elemento" del buffer puede ocupar uno o varios texels (p.ej., un punto 3D con atributos podría almacenarse en varios píxeles). Luego, un _shader_ puede acceder a ellos vía funciones de muestreo (texelFetch o imageLoad en una imagen 1D). Esto aprovecha la API existente (gpu.texture.from_image para imágenes, o GPUTexture((N,1), format) para texturas internas). La limitación es la cuantización a 4 componentes por texel; para estructuras más complejas habría que repartir en múltiples texturas o en más texels consecutivos.
- **Usar uniform buffers para pequeños conjuntos:** Blender sí permite uniformes del shader, pero están limitados en tamaño. Para datos pequeños (e.g. vectores de constantes), se podría concebir un nodo que suba un array como UBO (Uniform Buffer Object). Aun así, para cientos o miles de elementos esto no escala.

Cuando Blender exponga SSBOs en Python (posiblemente vía gpu.types.GPUStorageBuffer en un futuro), podremos mapear directamente un recurso IR tipo BUFFER a un SSBO GLSL (ej. layout(std430, binding=X) buffer BufName { ... };). Recomendamos diseñar la abstracción de **ResourceType** en el IR con esta futura incorporación en mente: por ejemplo, añadir ResourceType.BUFFER y opcodes IR BUFFER_READ/WRITE (ya definidos en el MVP[\[9\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/ir/ops.py#L100-L105)) que luego el generador de código traduzca a accesos SSBO cuando sea posible. En la ejecución, ComputeExecutor necesitaría crear/gestionar dichos buffers análogamente a las texturas. Hasta entonces, la solución de textura1D nos permite soportar casos importantes (partículas, arrays) de forma transparente al usuario - este simplemente verá un _socket_ de tipo "Buffer" o similar.

- **Soporte de texturas 3D (volúmenes):** Si tratamos volúmenes de densidad u otros grids 3D, aprovecharemos las texturas 3D de OpenGL. Blender **debería** soportar la creación de GPUTextures 3D (dado que OpenGL 4.3 lo permite y Vulkan también). Confirmaremos vía gpu.types.GPUTexture((size_x, size_y, size_z), format=…) si es viable. En caso afirmativo, TextureManager.ensure_internal_texture puede ampliarse para manejar la dimensión extra. El IR ImageDesc para un volumen contendrá size=(X,Y,Z). En GLSL generaremos image3D. Un aspecto a vigilar es el **filtro**: si en un shader 2D se usa un volumen 3D, ¿cómo se muestrea? Podríamos restringir cada shader a una dimensión uniformemente (así, un volumen solo se procesa en un pass 3D). Para convertir un volumen a imágenes 2D (slices) o viceversa, se ofrecerían nodos especializados (p. ej. un nodo que extrae un corte 2D de un volumen).

**Conclusión de kernels 1D/2D/3D:** Con estos cambios, ComputeNodes podrá ejecutar **kernels de distinta dimensionalidad**, alineándose con Houdini Copernicus donde los nodos de imagen se consideran esencialmente "volúmenes 2D" (lo que hace natural extender a volúmenes 3D y datos 1D)[\[10\]](https://tokeru.com/cgwiki/HoudiniCops.html#:~:text=%2A%20New%20cops%20flow%20left,back%20to%20the%20viewport%20live). El plan prioriza usar texturas 1D/3D como análogo a SSBO/volumen mientras la API no provea algo específico, minimizando la dependencia en características futuras pero dejando abierta la extensibilidad a SSBOs reales cuando estén disponibles.

## **Bucles Avanzados y "Repeat Zones" (Ping-Pong, Cascada de Resoluciones)**

El MVP implementa una forma básica de bucle mediante los nodos **Repeat Zone (Input/Output)**, pero con limitaciones: solo maneja un valor escalar "Current Value" que itera, asumiendo un bucle lineal sobre todos los píxeles con un número fijo de iteraciones[\[11\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/nodes/control_flow.py#L114-L123)[\[12\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/nodes/control_flow.py#L132-L139). Actualmente, al extraer el gráfico se inserta directamente una secuencia IR con opcodes LOOP_START y LOOP_END que engloban las operaciones internas[\[3\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/graph_extract_old.py#L801-L810)[\[13\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/graph_extract_old.py#L838-L847). Para robustecer este sistema necesitamos:

- **Bucles con múltiples variables y tipos:** En la versión actual, RepeatInput produce dos salidas: Iteration (int) y Current Value (float), y RepeatOutput acepta Next Value (float)[\[14\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/nodes/control_flow.py#L115-L123)[\[12\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/nodes/control_flow.py#L132-L139). Esto está _hardcodeado_ para manejar esencialmente una suma acumulativa de un escalar. Queremos que un bucle pueda transportar **cualquier tipo de dato** (imágenes, vectores, estructuras) de una iteración a la siguiente, similar a los "feedback loops" en sistemas de nodos avanzados. Para lograrlo:
- Rediseñaremos los nodos de bucle para que soporten **múltiples pares de sockets** de entrada/salida en Repeat Input/Output. Por ejemplo, el usuario podría configurar un Repeat Input con N "Initial" entradas y N "Current" salidas (hoy fijo en 1 par), permitiendo varias variables de estado. Podemos exponer un UI dinámico: un botón "+" para agregar otro par de sockets al loop (similar a cómo funciona el nodo _Combine_ en Geometry Nodes).
- **Tipos de datos arbitrarios:** en lugar de forzar float, las salidas del Repeat Input deben heredar el tipo de sus iniciales. Si se quiere iterar una imagen, el socket "Initial Image" producirá un "Current Image". En IR, eso implicará que LOOP_START acepte valores de tipo _handle_ (imagen) además de floats. El IR actual marca LOOP_START/END sin distinción de tipo - podríamos extender la semántica para manejar vectores o handles. En la práctica, un **bucle sobre imágenes** sería un ping-pong buffer trivial (alternar dos texturas) a menos que combinemos con ping-pong (ver siguiente punto).
- El sistema IR/Codegen deberá soportar outputs múltiples de LOOP_START. Ahora mismo ya añade dos outputs (valor actual y índice) manualmente[\[15\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/graph_extract_old.py#L813-L822); generalizarlo a N outputs es factible: crear M outputs SSA para LOOP_START (uno por variable) más el índice, y en LOOP_END consumir M+1 valores (las "Next" de cada variable más el acumulador previo). Ajustaremos los _emitter_ GLSL para generar un bucle for donde se actualizan todas las variables simultáneamente.
- **Patrón Ping-Pong (doble buffer):** Muchas iteraciones en procesamiento de imágenes alternan escribir y leer de dos buffers para evitar sobreescribir datos que se necesitan en el siguiente paso. Queremos exponer esta capacidad al usuario. Una forma amigable es un **nodo especial de Ping-Pong** o una opción en los nodos Repeat:
- **Opción A:** Introducir un nodo PingPong Loop que internamente se configure con dos imágenes de trabajo. El usuario provee la imagen inicial A, y dentro del bucle las operaciones escriben en B; al final de cada iteración, se intercambian (lo que era B pasa a A para la siguiente iteración). Este nodo podría ser un meta-nodo que genere el IR apropiado (similar a Repeat, pero con dos recursos intercambiables).
- **Opción B:** Permitir que el par RepeatInput/Output detecte recursos de tipo imagen y automáticamente active modo ping-pong. Por ejemplo, si la "Current Value" es de tipo imagen, podemos en IR asignarle dos ImageDesc de salida e ir alternando en LOOP_END: un ciclo escribe a la textura1, el siguiente a textura2. Esto requiere que el IR LOOP_START/END sepan de doble-buffering. Alternativamente, modelar ping-pong explícitamente: Podríamos introducir opcodes LOOP_SWAP o aprovechar ResourceAccess.READ_WRITE en el descriptor para indicar que se usa en ambos roles. Una implementación viable: **mantener dos índices de recurso** en el IR para la misma entidad lógica y conmutarlos en cada iteración mediante una operación especial (p.ej., un XOR de índice en GLSL).
- Desde el punto de vista del _shader_, implementar ping-pong puramente dentro de un solo shader es complicado (porque un solo dispatch no puede "alternar" buffers iterativamente, salvo usando barreras y grupos de trabajo que reinvocaríamos manualmente, lo cual sale del modelo). Es más sencillo concebir que **cada iteración es un pass separado**: el _scheduler_ podría "desenrollar" un ping-pong loop en passes alternos (aunque con N iteraciones fija, se puede generar 2N passes alternando recursos, pero eso no escala si N es grande). En su lugar, se podría simplemente ejecutar el mismo compute shader N veces desde Python, alternando la vinculación de texturas. **Recomendación:** Para ping-pong de iteraciones muchas, usar la vía de _dispatch_ múltiple (con pequeñas modificaciones al ComputeExecutor para bucles), en lugar de intentar meterlo todo en un solo shader con lógica de índice. Esto implicaría que al extraer IR de un PingPong Loop, detectamos el patrón y no generamos un loop GLSL, sino un **bloque repetitivo en CPU** que realiza dispatch N veces. Se perdería algo de paralelismo global, pero se gana en simplicidad y menor consumo de VRAM (solo dos buffers reutilizados).
- De cara al usuario, habilitar ping-pong significa permitir algoritmos iterativos como blur difusivo, erosiones/dilataciones repetidas, etc., donde sin esta técnica se requerirían duplicar manualmente nodos para simular iteraciones.
- **Cascada de resoluciones (multi-scale):** Este concepto implica procesar la imagen en múltiples escalas (por ejemplo, generar una pirámide Gaussiana, procesar de baja a alta resolución). En Houdini/Nuke se suele lograr con nodos de _scale_ combinados con bucles. Para incluirlo:
- Añadir un nodo (o integrar en Repeat) que **varíe la resolución en cada iteración**. Por ejemplo, un nodo Loop (Multi-Scale) podría tener factores de escala (p.ej., 50% cada iteración). Internamente, cada iteración crearía/copiaría la imagen a una resolución reducida. Implementarlo dentro de un solo shader es complejo; lo natural es orquestarlo a nivel de passes:
  - Estrategia: Suponer que en la primera iteración usas la imagen full res, en la segunda una reducida al 50%, etc. El _ComputeExecutor_ podría reconfigurar el tamaño de dispatch y texturas para cada iteración. Esto requiere poder **recrear o redimensionar GPUTextures sobre la marcha**. Una forma sería crear desde el inicio todas las texturas de las resoluciones necesarias (según un factor o una lista dada) y almacenarlas en TextureManager. El bucle iría asignando val_target.size diferente antes de cada dispatch.
  - Alternativamente, podríamos generar en IR un _tree unrolling_ de resoluciones: ej. un loop de 3 iteraciones a 100%,50%,25% se convierte en tres subgráficos conectados donde cada uno hace un resize + proceso. Sin embargo, eso pierde generalidad y elegancia.
- Quizá es más sencillo ofrecer nodos básicos de **Scale/Resize** (p. ej. Scale Image con factor o resolución absoluta) y permitir al usuario combinarlos con loops manualmente. Un nodo _Scale_ generaría en IR una operación que crea un nuevo ImageDesc con dimensiones diferentes y llena sus píxeles muestreando la imagen de entrada (posiblemente usando hardware bilinear via sampler). De hecho, implementar un _downscale_ eficiente puede requerir un shader específico (no trivial con un solo imageStore por pixel; podríamos aprovechar texturas y sampler2D con filtrado si Blender GPU lo permite en compute).
- Dado el tiempo, recomendamos inicialmente una **solución explícita**: incluir un nodo _Resize_ o _Resample_ para que el usuario construya la cascada manualmente (como: Imagen -> Resize 50% -> ...). Más adelante, podríamos añadir azucar sintáctico mediante un tipo especial de loop que incorpore internamente ese resize cada iteración.
- **Integración con IR y planner:** Los bucles actuales se representan en IR con OpCode.LOOP_START/END[\[16\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/ir/ops.py#L78-L86) y manejan la iteración internamente en el shader (se genera un for loop). Con las mejoras propuestas, especialmente para ping-pong y multi-res, puede que optemos por resolver ciertas repeticiones a nivel de _scheduler_ (como múltiples passes en serie). Esto significa que _no siempre utilizaremos un loop GLSL interno_; en ocasiones, el bucle de alto nivel se traducirá a ejecutar varios _ComputePass_ secuenciales. **Importante:** mantener consistencia para el usuario: el nodo loop sigue siendo uno, pero bajo el capó podría engendrar varios passes. El _planner/scheduler_ deberá detectar construcciones de IR especiales (p.ej., un LOOP_START marcado como ping-pong o multi-res) y en vez de dejarlo dentro de un pass, segmentarlo.
- Por ejemplo, podría introducirse un nuevo opcode LOOP_END_PASS que indique "termina la iteración y requiere un nuevo dispatch con resultados parciales", señal para el scheduler de cortar el pass en ese punto.
- Otra opción es no complicar el IR con eso y manejarlo puramente en execute_compute_tree: detectar nodos loop en el árbol Blender antes de IR, y si es de tipo ping-pong o multi-res, ejecutar manualmente dentro de la función Python repetidamente. Esto es más procedural y rompe el ideal declarativo del IR, pero simplifica la implementación en corto plazo. **Recomendación:** Para ping-pong y multi-res, inicialmente implementar la lógica en la capa de ejecución Python (más control y posibilidad de debug), e ir migrando a IR/GLSL nativo conforme la necesidad de optimización lo dicte.

En resumen, estas mejoras a los bucles proporcionan construcciones para **iteración más poderosa**: desde loops multi-variable hasta patrones de feedback y procesado piramidal. Esto acerca el sistema a Houdini Copernicus, donde existen nodos análogos (por ejemplo, _Block Begin/End_ para construir bucles o "for-each" logic) y donde los usuarios pueden configurar loops complejos dentro del flujo de nodos. Nuestro diseño busca un balance entre incorporar estas capacidades y mantener claridad de uso. Un **riesgo** a señalar es la mayor complejidad en la ejecución: ejecutar muchos iteraciones o escalas puede ser costoso; habrá que proporcionar indicaciones visuales (por ejemplo, limitar _Auto-Execute_ cuando hay loops intensivos, para no recalcular 100 iteraciones en cada cambio trivial). Pero la modularidad del nuevo diseño nos permitiría optimizar o paralelizar en GPU en el futuro estas iteraciones pesadas.

## **Soporte Ampliado de Tipos de Datos: Partículas, Point Clouds, Volúmenes, Datos Arbitrarios**

Llevar el addon más allá de imágenes 2D implica abrirlo a otros dominios de datos. En línea con la filosofía de Copernicus (que integra COPs con geometría y volúmenes)[\[10\]](https://tokeru.com/cgwiki/HoudiniCops.html#:~:text=%2A%20New%20cops%20flow%20left,back%20to%20the%20viewport%20live), proponemos:

- **Integración con nubes de puntos/partículas:** Introducir nodos capaces de **leer y escribir datos de partículas** o point clouds de Blender. Por ejemplo:
- _Nodo Input Particles_: toma un sistema de partículas (o un objeto PointCloud) de la escena y produce un **recurso buffer** con los atributos (posición, velocidad, etc) de cada partícula. Internamente, esto significaría crear un ResourceDesc de tipo BUFFER y copiar los datos CPU->GPU (posiblemente usando NumPy para empaquetar atributos en un array, luego subiéndolo a la textura 1D o SSBO).
- _Nodo Output Particles_: tomar un buffer (tras cálculos) y aplicarlo de vuelta a un sistema de partículas o PointCloud. Esto es más complejo pues requiere copiar GPU->CPU. La API gpu probablemente permita leer de un GPUTexture (texture.read() quizás devuelva un array) - habría que confirmar. En caso afirmativo, se mapearía ese array a las posiciones/atributos de las partículas en Blender.
- Mientras SSBO no esté accesible, este mecanismo usando texturas 1D funciona, aunque la transferencia CPU/GPU pueda ser un cuello de botella. Un posible _mitigador_ es permitir que ciertos cálculos de partículas se hagan totalmente en GPU sin bajar a CPU hasta el final (por ejemplo, encadenar varios nodos de partículas en GPU y solo al final volcar el resultado).
- Cabe señalar que Blender 4.x introdujo **Simulation Nodes** en Geometry Nodes para física de partículas; nuestro sistema podría complementarlo ofreciendo cálculos arbitrarios en GPU. Sin embargo, hay riesgo de duplicar funcionalidad; debemos enfocarnos en usos donde ComputeNodes brinde aceleración o flexibilidad extra (shaders custom, grandes volúmenes de partículas, etc.).
- **Soporte de volúmenes (grids 3D):** Similar al caso de imágenes 2D, pero ahora con **Voxel Data**. Podríamos tener:
- _Nodo Volume Input_: toma un objeto Volume de Blender (OpenVDB) y crea una textura 3D (o un buffer 3D) con sus valores. Dado que OpenVDB no es denso, habría que muestrear a una resolución 3D fija. Quizá más práctico es soportar los volúmenes densos nativos de Blender (que son como cuadrículas en mantaflow).
- _Nodo Volume Output_: escribe los voxeles procesados de regreso a un objeto Volume o crea uno nuevo.
- Estos nodos implican manejar posiblemente **grandes cantidades de datos**, así que habrá que vigilar memoria. A diferencia de imágenes (donde 4K^2 pixeles ~ 16 millones), un volumen 256^3 ya son ~16 millones de voxels, tratables pero 512^3 son 134 millones (cuidado). Copernicus probablemente maneja volúmenes como geometry (VDB) en lugar de texturas completas, aprovechando sparsidad. Nuestro enfoque inicial será para volúmenes "pequeños" o moderados, usando texturas 3D.
- Desde la óptica IR, esto ya queda cubierto con ResourceType.IMAGE + dimension=3D como se discutió. Añadiríamos opcodes si es útil (por ej, operaciones específicas de volúmenes como _voxel slice_, pero puede resolverse con nodos compuestos de básicos).
- **Datos arbitrarios / estructuras personalizadas:** Sería ideal permitir al usuario manejar datos no previstos, por ejemplo tablas numéricas, resultados de simulaciones científicas, etc. Esto casi seguro requerirá **Shader Storage Buffers** (o workaround vía textures) como ya analizamos. Una idea es exponer un nodo tipo _Buffer Input_ que permita al usuario cargar datos desde un archivo CSV/EXR (por ejemplo) o ingresarlos manualmente. Sin embargo, esto sale un poco del ámbito típico de un addon nodal para gráficos. Quizá más relevante es:
- Soportar **imágenes multi-capa** o _deep data_: Copernicus tiene noción de "cables" con múltiples canales[\[17\]](https://www.sidefx.com/docs/houdini/nodes/cop/index.html#:~:text=Wiring%20COPs%20together%20controls%20the,nodes%20that%20modify%20the%20data)[\[18\]](https://www.sidefx.com/docs/houdini/nodes/cop/index.html#:~:text=Image), por ejemplo un paquete de imágenes juntas. En nuestro diseño podríamos mapearlo a recursos tipo BUFFER donde se concatenan varias capas, o bien a texturas 2D con más componentes (p.ej. RGBA, o usar varias texturas). Esto permitiría manejar datos arbitrarios asociados a cada pixel (más allá de RGBA). Un caso de uso: almacenar una profundidad Z por pixel junto a color - esto en Copernicus podría ser un "layer" extra en el cable. Aquí podríamos implementar nodos _Combine Layers_, _Separate Layers_ que empaqueten/desempaqueten información. Por simplicidad, podemos por ahora limitarnos a RGBA (ya soportado), pero dejar arquitectura preparada para extensiones (p.ej., ImageDesc tiene un campo format donde ya reconocemos RGBA8, RGBA16F, R32F, etc.[\[19\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/codegen/glsl.py#L64-L72); podríamos añadir RG32F o similares para otros canales).

En la **arquitectura IR**, la introducción de nuevos tipos de recurso (BUFFER, VOLUME) se traduce en más entradas en el enum ResourceType y ajustarle la lógica donde corresponda (p.ej., la generación de binding en GLSL deberá distinguir entre imagen y buffer; actualmente solo hace binding de imágenes[\[20\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/codegen/glsl.py#L56-L65)[\[4\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/codegen/glsl.py#L88-L96)). Probablemente gestionemos buffers sin declararlos en el shader (pues Blender podría requerir usar shader.buffer(...) en lugar de shader.image(...) para vincularlos - detalle a investigar en API). En caso extremo de no poder declarar SSBO, una idea es usar una \*imagen 1D\* pero accederla víashader.image("Image_0", bufTexture)\` igual que imágenes (sería simplemente otra textura bind, que en GLSL se ve como image2D de width=N, height=1). Esto aprovecharía el mismo mecanismo de binding actual sin cambios drásticos.

**Limitaciones y riesgos:** Soportar estos tipos de datos incrementa la complejidad de sincronización CPU-GPU y el uso de memoria GPU. Copias grandes de ida/vuelta (p.ej. millones de partículas cada frame) pueden anular la ventaja del cómputo GPU. Habrá que documentar claramente que el mejor rendimiento se logra manteniendo los datos en GPU el mayor tiempo posible (por ejemplo, hacer varias operaciones encadenadas en ComputeNodes y solo al final recuperar el resultado). Otro riesgo es la **complejidad de usuario**: Blender ya tiene Geometry Nodes para puntos/volúmenes; habrá que justificar cuándo usar ComputeNodes (por ejemplo, para escribir un shader personalizado que afecte millones de puntos más rápido que Geometry Nodes en CPU, o para operar volúmenes con fórmulas complicadas). A nivel técnico, debemos hacer pruebas en distintas GPUs para asegurarnos de que manejar texturas 3D o buffers grandes no cause caídas de rendimiento inesperadas (por ej., algunas GPU podrían no soportar bien _imageLoad_ en texturas muy grandes - quizás debamos implementar particionado/tiled processing en el futuro, aunque es avanzado).

## **Diseño Modular y Nodos de Scripting Personalizado (GLSL in-node)**

La modularidad del sistema es crucial para mantenerlo extensible y mantenible. El MVP ya separa varias capas: definición de nodos Blender, construcción de IR (antes centralizado en una sola función, ahora migrando a _handlers_ modulares[\[21\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/graph_extract/registry.py#L10-L18)), generación de código GLSL mediante _emitters_ registrables[\[22\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/codegen/glsl.py#L168-L176) y ejecución mediante gestores de recursos y shaders. Identificamos varias mejoras de diseño:

- **Refactor de extracción IR y registro de handlers:** La existencia de graph_extract_old.py indica que el MVP inició con una función monolítica para traducir nodos a IR[\[23\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/graph_extract_old.py#L17-L25)[\[24\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/graph_extract_old.py#L35-L44). Ahora se pretende usar un registro por tipo de nodo (HANDLER_REGISTRY mapeando bl_idname a función)[\[25\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/graph_extract/registry.py#L11-L19). Aseguraremos que cada tipo de nodo tenga su handler bien definido, reduciendo _hardcodes_. Por ejemplo, en el MVP la extracción del nodo Output no aparecía en el registro, sino que se manejaba al final ad-hoc[\[26\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/graph_extract_old.py#L876-L885)[\[27\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/graph_extract_old.py#L905-L914). Proponemos convertir eso también en un handler (p.ej. handle_output_node) que encapsule la lógica de crear el ImageDesc de destino (si no viene de otro nodo) y generar la operación de escritura (IMAGE_STORE). Así evitamos código especial disperso. Igualmente, la lógica de bucles puede modularizarse: en lugar de anidar recursivamente process_node como ahora, un handler handle_repeat_output debería construir las ops IR de loop correspondientes[\[28\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/graph_extract_old.py#L8-L11), mejorando legibilidad. Este enfoque **facilita añadir nuevos nodos**: solo se escribe una función de manejo y se registra, sin tocar la máquina de extracción central.
- **Sistema de generación GLSL extensible:** De modo análogo, la generación de código usa emisores registrables (funciones lambda que construyen código por opcode)[\[22\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/codegen/glsl.py#L168-L176). Hoy existen _emitters_ para la mayoría de opcodes (math, vector, etc.). Hay que asegurarse de que agregar un nuevo opcode sea simple: actualizar el enum OpCode y registrar su emitter. Un potencial problema es que algunos nodos complejos generan _secuencia_ de ops (no una sola). Por ejemplo, _Separate Color_ produce varios outputs; su emitter actualmente necesita declarar múltiples variables de salida manualmente en GLSL (por eso está en self_declaring_ops para tratamiento especial)[\[29\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/codegen/glsl.py#L148-L156). Podríamos mejorar esto permitiendo que un emitter opcionalmente devuelva no solo código sino también instrucciones de declaración extra. Otra alternativa es **introducir plantillas** para conjuntos de operaciones frecuentes: p.ej., muchas operaciones matemáticas componente a componente se parecen. Podríamos factorizar un emitter genérico para binarios (a op b) que reciba el símbolo operatorio. Aún así, dado que la lista de opcodes es finita, mantener los emitter explícitos es aceptable y claro.
- **Nodo de scripting GLSL in-node:** Esta es una funcionalidad potente para usuarios avanzados: un nodo donde puedan **escribir código GLSL personalizado** que se inserte en el shader. Equivalente a un "Script Node" (similar al nodo OSL Script en Cycles, pero aquí en GLSL para Eevee/Compute). Para implementarlo:
- Crearemos un nuevo nodo Blender, e.g. ComputeNodeScript con un **propiedad de texto multilinea** (o un enlace a un Text datablock de Blender para edición más cómoda). En el dibujo del nodo, mostraremos un pequeño editor o al menos el nombre del script vinculado.
- En la extracción IR, este nodo se manejará de forma especial: deberá **generar IR dinámico**. Podemos optar por introducir un opcode genérico tipo OpCode.CUSTOM para representarlo, con atributos que incluyan el código fuente y metadatos de entradas/salidas. Sin embargo, quizás no valga la pena pasarlo por IR detallado; podríamos simplemente encapsularlo hasta la generación GLSL.
- Propuesta: El handler del Script Node creará _Values_ IR para cada salida del nodo (como placeholders), pero no agregará ops regulares. Luego, en la fase de generación GLSL, detectaremos este nodo y **insertaremos su código fuente**. ¿Cómo? Dos maneras:
  - **Inline en main():** Tomar el código escrito e inyectarlo en el cuerpo del shader, antes de usar sus resultados. Por ejemplo, si el usuario escribe algo como vec3 col = inColor \* vec3(2.0); outColor = vec4(col, 1.0);, podríamos insertarlo tal cual con las variables correspondientes ya definidas.
  - **Como función auxiliar:** En vez de inline, podríamos envolverlo en void nodeFunc(vecX input1, ..., out vecY out1, ...) { ... } y llamarla desde main. Esto sería más limpio si el código es extenso y reutilizable. Pero para simplicidad inicial, inline está bien.
- Debemos proveer al código del usuario los **nombres de variables de entrada**. Proponemos una convención: por ejemplo, el Script Node con N entradas podría exponerlas como in0, in1, ..., inN en su código, y las salidas como out0, out1, .... El addon, al generar, reemplazará esos identificadores con los verdaderos nombres de variables SSA en GLSL. Por ejemplo, si in0 corresponde a un Value %5 (v5 en GLSL) y out0 corresponde a %6, podríamos hacer: vec3 v6 = /\*user code with v5\*/;. Alternativamente, podríamos pre-generar líneas antes: vec3 in0 = v5; luego el código, luego asignar v6 = out0;. Cualquier método requiere _parsear_ ligeramente el texto del usuario o especificar claramente cómo debe formatearlo. Para facilitar, podemos pedir que el usuario devuelva directamente el resultado, p. ej.: _"outColor = ...;"_. En cuyo caso, nuestro engine puede simplemente injertar su código dentro del main() y confiar en que asigna a variables nombradas outColor que nosotros habremos declarado.
- **Seguridad y validación:** Una preocupación es que un error en el código del usuario generará un fallo de compilación del shader. Debemos capturar el log de compilación (Blender _GPUShader_ suele proveer el log de compilación si falla) y reportarlo al usuario. Podríamos utilizar shader.validate() si existe, o envolver la creación con try/except y extraer el mensaje. Luego, ese mensaje se podría mostrar en la interfaz, quizás marcando el nodo en rojo y poniendo el error en su label.
- **Limitaciones del script node:** Para mantenerlo manejable, inicialmente restringiremos que solo opere con tipos escalares o vectores "por pixel". Es decir, no deberá por sí mismo llamar a _imageLoad_ ni realizar accesos a recursos externos - esas operaciones seguirán hechas vía nodos específicos (Input Image, Sample, etc.) cuyos resultados puede tomar como inN. Esto porque si permitimos acceso arbitrario a imágenes dentro del código, saltaríamos el IR y planificador, pudiendo introducir hazards no controlados. Si un usuario experto realmente quiere hacer algo muy custom (por ejemplo, un filtro que lee vecinos de un pixel), _podrá hacerlo_ combinando: usar un Position node para obtener coord, luego dentro del script usar funciones GLSL con esa coord para samplear una textura. De hecho, necesitamos exponer quizás algún mecanismo para acceder a texturas dentro del script. Una opción sencilla es permitir que _entradas de imagen_ en el script node generen directamente un uniform sampler2D accesible. Podríamos mapear un socket de tipo ComputeSocketImage en el Script Node a un uniform readonly image2D Image_N. Sin embargo, eso es complejo porque implicaría que el usuario escriba por ejemplo vec4 px = imageLoad(Image_0, ivec2(x,y)); dentro de su código. Esto puede hacerse si documentamos que Image_0 estará disponible (tal como lo hacemos con shader.image("Image_0", tex) en la API[\[30\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/tests/test_runtime.py#L142-L150)). Por ahora, quizá limitamos script node a operaciones matemáticas sobre sus inputs numéricos; más adelante podríamos permitir que reciba imágenes y use sus valores mediante alguna API (tal vez inyectando también el código de muestreo).

En definitiva, el **nodo de script** agrega enorme flexibilidad: los usuarios podrán implementar operaciones no previstas (ej: un filtro artístico, una función matemática especial) escribiendo unas líneas de GLSL en lugar de esperar a que el addon las incorpore. Esto acelera la adopción por _power users_. El principal riesgo es la **usabilidad**: no todos los artistas saben GLSL, y exponerlo podría ser intimidante. Sin embargo, al tratarse de un addon especializado, es aceptable que sea una característica orientada a TDs o desarrolladores técnicos. Otro riesgo menor es que cambios en la API GPU de Blender (por ejemplo, con la transición a Vulkan y uso de SPIR-V) puedan requerir ajustar la sintaxis esperada en estos scripts. A corto plazo, mantendremos GLSL como lenguaje (que Blender seguirá aceptando y compilando detrás de escena[\[31\]](https://devtalk.blender.org/t/python-shadercreateinfo-missing-in-blender-4-5-what-s-the-plan/42007#:~:text=plan%3F%20devtalk,don%27t%20work%20under%20Vulkan)) y monitorearemos cualquier deprecación.

- **Puntos de extensibilidad y modularidad adicional:** Con la infraestructura de handlers y emitters, agregar un **nuevo nodo nativo** (no script) será bastante directo. Podemos documentar un procedimiento para ello, e incluso pensar en permitir **plugins externos** que registren nodos en ComputeNodes. Por ejemplo, si un desarrollador quiere agregar un conjunto de nodos para cierto efecto (AI denoising, etc.), idealmente podría sin modificar el core, registrar más NodeItems y asociar sus funciones IR/GLSL. Para soportar eso, conviene que el registro de handlers y emitters esté accesible públicamente (p.ej. exponer compute_nodes.graph_extract.get_handler y permitir hacer HANDLER_REGISTRY\["MyNode"\] = my_handler). Similar con emitters. Debemos, no obstante, cuidar que no se rompa encapsulamiento: quizás ofrecer una API oficial como compute_nodes.register_custom_node(node_class, handler_func, emitter_func). Esto realmente convertiría el addon en una **plataforma extensible** dentro de Blender.
- **Solución de inconsistencias de diseño actuales:** Aprovecharemos la revisión modular para corregir algunos problemas detectados:
- El **nodo Output** actualmente tiene propiedades width, height, format pero en la extracción original no se usaban si no había imagen de destino conectada (no se creaba la ImageDesc automáticamente)[\[32\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/graph_extract_old.py#L880-L889)[\[27\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/graph_extract_old.py#L905-L914). Corregiremos esto haciendo que si ComputeNodeOutput.image (socket de entrada) no está ligado a nada, el handler cree un ImageDesc nuevo con el nombre/size/format del nodo Output. De ese modo el output siempre tiene un recurso válido. Este cambio eliminará la necesidad de tener un nodo "ImageWrite" separado para simplemente definir la imagen final - podríamos incluso deprecar ComputeNodeImageWrite y quedarnos solo con Output manejando la creación (o redefinir ImageWrite como un alias de Output con menos opciones).
- **Dynamic socket update issues:** Algunos nodos (Switch, Mix, Separate Color, Combine Color, Map Range) cambian sus sockets según propiedades (tipo de dato, modo de color, etc.)[\[33\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/nodes/control_flow.py#L30-L38)[\[34\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/nodes/converter.py#L53-L61)[\[35\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/nodes/converter.py#L149-L157). Debemos verificar que esos cambios se reflejan correctamente en el IR. Actualmente, la extracción de nodos toma sockets por nombre; si un socket está oculto (socket.hide=True) pero sigue existiendo, igual se consideraría. En el handler habría que respetar qué entradas están activas. Posible mejora: hacer que los handlers usen los mismos criterios que el UI (p.ej., handler de Mix consultará la propiedad .data_type del nodo y solo procesará los sockets relevantes). Esto previene p.ej. que un Mix en modo Vector intente sumar floats porque quedó un socket residual oculto.
- **Operaciones lógicas y comparaciones:** El IR define opcodes como AND, OR, EQ, LT, etc.[\[36\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/ir/ops.py#L66-L74) pero en el MVP no hay nodos de lógica dedicados (aunque un Compare podría mapear a EQ/GT...). Decidiremos si agregar nodos lógicos (podría ser útil en conjunción con Switch) o si dejarlo para el usuario vía Script node. En todo caso, limpiar lo que esté definido pero no usado para mantener consistencia.
- **Gestión de formatos numéricos:** Hoy casi todo es float32 en GPU (salvo se mencionan UINT/INT en DataType[\[37\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/ir/types.py#L4-L13)). Si quisiéramos soportar _integer images_ (RGBA8I/UI), la IR y codegen ya prevé algunos formatos[\[38\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/codegen/glsl.py#L70-L73). Podemos exponerlo en la UI (por ejemplo, permitir al usuario elegir formato entero en Output). Pero esto introduce la necesidad de manejar tipos en operaciones (no todas las ops admiten ints en GLSL sin conversión). Por ahora, podríamos documentar que el pipeline trabaja principalmente en float (como ocurre en Compositor tradicional), y soportar INT en casos puntuales. Modularmente, eso significa que en IR DataType.INT existe pero debemos usarlo con cuidado. Un _target_ futuro es permitir imágenes de máscara (8-bit) y datos en entero sin perder precisión, pero se sale un poco del foco inmediato.

En síntesis, este rediseño modular hace al sistema **más fácil de expandir y personalizar**. Los nodos de scripting brindan un _escape hatch_ para casos especiales, mientras que los registros de handlers/emitters aseguran que internamente las nuevas funciones no introduzcan espagueti. Un riesgo de tanta modularidad es la sobrecarga en rendimiento (llamadas Python extra, etc.), pero en general la mayor parte del trabajo sucede en GPU; la extracción IR/compilación shader ocurre al ejecutar el árbol (no por pixel), por lo que unos pocos niveles de indirección Python no serán problema. Mantendremos pruebas unitarias (como ya hay un test_runtime.py básico[\[39\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/tests/test_runtime.py#L85-L93)[\[40\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/tests/test_runtime.py#L117-L125)) para corroborar que la caché de shaders, creación de texturas, etc., siguen funcionando con estos cambios, y ampliaremos esos tests a nuevos nodos.

## **Dominios y Resolución: Comparativa con Houdini Copernicus, Debilidades y Mejora de Diseño**

En sistemas de composición avanzados (Nuke, Houdini COPs/Copernicus), el manejo de la **resolución de imágenes** y sus **dominios** es fundamental. Actualmente, ComputeNodes sigue un enfoque simplificado: **una única resolución global por nodetree**. Identifiquemos las diferencias clave con Copernicus y propongamos mejoras:

- **Resolución global fija vs. resoluciones por nodo:** En el MVP, al ejecutar el gráfico se determina un tamaño de imagen "contexto" - generalmente tomado del nodo Output o de la primera imagen de escritura encontrada[\[1\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/operators.py#L46-L55). Todo el _compute shader_ corre en ese tamaño (usando gl_GlobalInvocationID.x/y hasta width/height) y se asume que las operaciones se aplican a ese lienzo[\[41\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/graph_extract_old.py#L920-L928). Esto significa que **todas las imágenes involucradas se consideran de ese tamaño**, lo cual es una suposición muy fuerte. Si un Input Image tiene distinta resolución, actualmente no hay lógica para ajustar eso: probablemente sería muestreada fuera de rango o habría que confiar en que el usuario escaló la imagen externamente. En Copernicus, en cambio, **cada nodo puede operar a su resolución nativa** (cada COP es como una imagen/volumen con su propia transform/voxel size). De hecho, Copernicus representa imágenes como _geometría 2D_ y las combina en el espacio 3D - es decir, no fuerza a que todas tengan igual tamaño, sino que realiza transformaciones implícitas o explícitas al componer[\[10\]](https://tokeru.com/cgwiki/HoudiniCops.html#:~:text=%2A%20New%20cops%20flow%20left,back%20to%20the%20viewport%20live). Esta flexibilidad permite, por ejemplo, componer una imagen HD sobre un lienzo 4K offseteada, o procesar texturas pequeñas sin malgastar fillrate.
- **Debilidades del enfoque actual:** La falta de manejo de resoluciones múltiples se traduce en:
- **Imposibilidad de mezcla de imágenes de distinto tamaño**: no hay nodos para reescalar ni offsets, así que el usuario está limitado a usar imágenes ya del mismo tamaño.
- **Falta de nodos de transform**: no se contempló nodos para trasladar, rotar o escalar imágenes (comunes en compositing). Indirectamente, sin sistema de coordenadas flexible por imagen, estos nodos no existen.
- **No soporte de ROI (Regions of Interest)**: En compo es útil procesar solo una ventana de la imagen; aquí siempre se procesa la totalidad fija.
- **Cascada de resoluciones manual imposible**: Como se discutió, no se puede generar pirámides salvo cambiando la resolución global entre ejecuciones.
- **Desperdicio de cómputo**: Si una cadena de nodos produce un resultado final pequeño o un mask de baja res, igual estamos calculando a la máxima res global en cada paso, sobrecargando la GPU.
- **Propuesta de manejo robusto de dominio/resolución:** Tomando inspiración de Houdini:
- Introducir el concepto de **Dominio de Imagen** en el nodetree. Cada recurso de tipo imagen en IR ya lleva un size. En lugar de homogenizarlos inmediatamente al extraer el gráfico, conservaremos esos tamaños individuales en la IR y tomaremos decisiones de compatibilidad durante el planificado o la ejecución. Es decir, permitiremos que en un mismo Graph IR coexistan ImageDesc de distintos tamaños.
- **Reglas de combinación**: Cuando un nodo toma dos imágenes de distinto tamaño, ¿qué hacer? Podemos definir comportamiento por defecto y permitir anularlo:
  - Por defecto: Adoptar el dominio del **árbol principal** (p. ej., del Output final) y _re-muestrear implícitamente_ las imágenes más pequeñas o más grandes a ese tamaño cuando se usen. Esto es simple pero oculta procesos. Copernicus probablemente no reescala implícitamente sin avisar; usualmente hay nodos para eso. Quizá mejor opción:
  - Requerir un nodo explícito de **Reformat/Resize** para cambiar resoluciones. Es decir, si el usuario conecta una imagen 512x512 a otra de 1024x1024 en un Mix, podríamos: o bien automatizar (suponer que la 512 se estira centrada), o marcar un aviso de _mismatch_ recomendando insertar un nodo Resize. Blender Compositor tradicional opta por el ajuste implícito (toma por defecto la resolución del primer input del nodo compositor y reescala el segundo).
  - Para mayor control profesional, inclinamos por la vía explícita: el usuario debe insertar un nodo _Resize_ o _Set Domain_ si quiere cambiar. Sin embargo, podríamos seguir la convención de Blender Compositor para familiaridad: e.g., en un Mix Node, considerar la imagen de mayor resolución como base y reescalar la menor (usando nearest or linear). Esto se podría implementar en el _handler_ del Mix: si detecta inputs de diferente tamaño, generar en IR una secuencia: tmp = resize(input_small, size_of_large) seguido del op mezcla. O más sencillo, en el shader del Mix samplear la chica con coordenadas normalizadas - esto se logra si se conoce ratio de tamaños.
  - **Recomendación de compromiso:** Implementar comportamiento por defecto (evitar bloquear al usuario): usar la resolución mayor entre inputs para los cálculos, reescalando internamente la otra. Y además, proveer un nodo _Resize_ para casos en que el usuario quiera un control específico (interpolación linear vs nearest, o elegir una resolución arbitraria).
- **Nodos de transformación y dominio:** Añadir nodos como _Translate_, _Scale (UV)_, _Rotate_ que modifiquen la posición de una imagen dentro de otra. En IR, esto no cambia la resolución de la imagen, sino cómo se calculan sus coordenadas de muestreo. Podemos implementar, por ejemplo, un nodo Translate que toma una imagen y una offset (dx, dy) y produce una nueva imagen del mismo tamaño que la de contexto, cuyo shader simplemente toma cada coordenada (x,y) y la muestrea de la imagen entrada en (x-dx, y-dy) (aplicando borde negro si sale fuera). Esto, junto con _Resize_, cubre la funcionalidad de mover imágenes de distintos tamaños por un lienzo común.
- **Multiples salidas finales:** Blender Compositor permite tener varias salidas (Composite, Viewer, File Output). Copernicus permite varios output COPs. En ComputeNodes podríamos permitir **varios nodos Output** en un mismo árbol, cada uno con su propia resolución/imagen destino. El IR Graph ya soporta múltiples recursos de salida. Actualmente el extractor solo busca uno[\[24\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/graph_extract_old.py#L35-L44), pero podemos adaptarlo para recoger todos los ComputeNodeOutput presentes. Cada output generaría una correspondiente IMAGE_STORE en IR hacia su recurso. El _scheduler/execute_ tendría que manejar quizás lanzar distintos shaders si sus resoluciones difieren o si necesitan separarse por dependencia (de hecho, _schedule_passes_ ya agrupará las ops; es posible que cada output termine en un pass separado si no están enlazados entre sí). Esto permitiría, por ejemplo, calcular a la vez una salida full HD y una miniatura en un solo nodetree.
- **Planificador consciente de resolución:** Extenderemos la función de _hazard detection_ para incluir **cambios de resolución** como criterio de división de passes. Ya notamos que en schedule_passes hay un TODO sobre "diferentes requerimientos de dispatch"[\[7\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/planner/scheduler.py#L20-L28). Implementaremos allí que si una op va a escribir a un recurso con tamaño distinto al actual contexto, se haga un corte de pass antes de esa op. En otras palabras, agruparemos en un pass todas las ops que comparten el mismo dominio, y si viene una op para otro dominio, empezamos un nuevo pass (y potencialmente insertamos una op de reformat implícito si arrastra datos).
  - Ejemplo: un tree genera primero una imagen en mitad de resolución, luego la usa para componer sobre full res. El planner podría crear Pass1 (hasta generar la imagen half-res), luego Pass2 (que lee esa imagen y lee otras a full res, componiendo en full res). Entre Pass1 y Pass2, se detecta que la imagen half-res es leída en un contexto full-res; el scheduler puede insertar una **etapa de upscale** (esto podría ser automático o dejado al shader de Pass2 como muestreo escalado). Quizá más limpio: hacer que Pass2 directamente muestree con coords normalizados la textura half (no necesita upscaling explícito, el shader lo maneja).
- **Actualización de ejecución:** El ComputeExecutor.execute_graph hoy determina un solo context_width, context_height y lanza el dispatch[\[1\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/operators.py#L46-L55)[\[42\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/operators.py#L64-L71). Con passes de distinta resolución, tendremos que recalcular el tamaño antes de cada dispatch. Nuestra implementación del _scheduler_ puede añadir esa info a cada ComputePass (un pass podría guardar pass_width, pass_height). De hecho, podríamos llenar esos campos al crear los passes: por ejemplo, al cortar passes por hazard o res, setear el tamaño al de la mayor _write_ en ese pass (o a alguna referencia). En ejecución, antes de hacer shader.bind() y compute.dispatch, dimensionar el dispatch con esos valores. Las texturas internas también deben prepararse con esas dimensiones (p.ej., si Output pide 1024x1024 y otra rama tiene 512x512, debemos haber creado ambos en TextureManager).
- **Persistencia de recursos entre passes:** Cuando se separen passes por resoluciones, asegurarse de que un recurso de menor res producido en Pass1 sigue accesible en Pass2. En el MVP, \_texture_mgr guarda texturas internas en un diccionario (\_internal_textures) por nombre[\[43\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/tests/test_runtime.py#L80-L88)[\[44\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/tests/test_runtime.py#L94-L101). Mientras usemos el mismo name para la ImageDesc, la textura se conservará. Al cambiar de pass, al hacer shader.image("Image_#", tex) se usará la misma GPUTexture previamente calculada. Esto significa que no hay que copiar manualmente nada; solo hay que garantizar que **no liberamos ni recreamos la textura small de nuevo**. El TextureManager actual tiene ensure_internal_texture(name, desc) que crea si no existe[\[44\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/tests/test_runtime.py#L94-L101); si ya existe no la duplica. Cuando produzcamos la half-res, llamaremos con su nombre una vez, quedará almacenada, y luego en el pass siguiente la pediremos como input. Esto ya funciona en MVP para casos simples de multi-pass (e.g., si hubiera un ImageWrite intermedio). Nos aseguraremos de usar identificadores únicos (quizá basado en nodo nombre o ID) para cada recurso intermedio para no colisión.

Con estas mejoras, **ComputeNodes soportará múltiples resoluciones de forma controlada**, reduciendo la brecha con herramientas profesionales. En **Houdini Copernicus**, los usuarios pueden tener copias de imágenes de distintos tamaños conviviendo y COPs que operan como "volúmenes 2D" que integran con geometría. Nuestro sistema no llega al punto de integrar con 3D viewport (Copernicus permite previsualizar composiciones en el contexto 3D)[\[45\]](https://www.sidefx.com/docs/houdini/nodes/cop/index.html#:~:text=COP%20nodes%20provide%20real,manipulation%20within%20a%203D%20space), pero en cuanto a lógica de tamaños estaremos más cerca: permitiendo diferentes resoluciones, transformaciones 2D, y nodos de reformat. Un posible riesgo es **mayor complejidad para el usuario novel**, pues tendrá que entender cuándo necesita un Resize o por qué una imagen se estira. Mitigaremos esto con comportamientos por defecto sensatos (reescalar al mayor tamaño, similar al compositor de Blender). También se recomienda proveer buenos _tooltips_ y documentación en la wiki del addon sobre manejo de resoluciones. En rendimiento, ofrecer dominios menores puede **mejorar performance** (p. ej. procesar un blur en 1/2 resol. por loop, luego upscaling, ahorra muchos shader invocations). No obstante, la fragmentación en múltiples passes añade overhead de lanzar varios shaders; habría que medir, pero normalmente merece la pena si se reduce dramáticamente el cómputo por pass.

## **Experiencia de Usuario (UX) y Coherencia con Sistemas de Nodos de Blender**

Para lograr adopción, ComputeNodes debe sentirse como "parte natural" del ecosistema Blender, encajando con Shader Nodes, Geometry Nodes y Compositor en términos de interacción y apariencia. Abordaremos varios aspectos UX:

- **Interfaz de nodos consistente:** Usaremos convenciones estándar de Blender:
- Mantener íconos y colores de nodos acordes a su función (ya se usan íconos de Blender e.g. IMAGE_DATA, TEXTURE, OUTPUT, etc. en bl_icon[\[46\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/nodes/input.py#L6-L14)[\[47\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/nodes/output.py#L14-L22)). Si añadimos nodos (Particles, Volume), buscaremos iconos adecuados (por ej. usar icono de punto para point cloud, etc.).
- Nombrado de nodos y propiedades en **idioma Blender**: por ejemplo, "Mix" en vez de "Lerp", "Value" en vez de "Scalar", etc., tal como ya está (ComputeNodeMath usa nombres coincidentes con el nodo Math estándar de Blender[\[48\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/nodes/math.py#L10-L19)). Esto reduce curva de aprendizaje al reutilizar conocimiento del usuario.
- Organización en categorías del menú Add Node similar a otros editores: ya se tiene Input, Texture, Math, etc.[\[49\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/categories.py#L10-L19). Agregaremos nuevas categorías si procede (quizá "Particles" o "Volumes" para esos nodos), manteniendo la estructura clara.
- Compatibilidad con **Frames y Reroutes**: Blender permite agrupar nodos en marcos y puntos de reroute. Nuestro nodetree debería soportarlos sin problema (heredados de NodeTree base). Verificaremos que arrastrar noodles, duplicar nodos (Shift+D), buscar nodos (menú Add > Search) funcionen correctamente dentro de nuestro espacio.
- **Nodo Group y encapsulación:** Una gran ventaja en Blender es la capacidad de hacer **Group Nodes** (sub-árboles reutilizables con interfaz simplificada). Deberíamos soportar que usuarios creen grupos dentro de ComputeNodeTree. Si Blender no lo activa por defecto para custom nodetrees, buscaremos cómo hacerlo (posiblemente setting \`bl_options = {'NODE_GROUP'} or similar en NodeTree class). Esto permite encapsular lógica compleja (por ej. un bucle multi-step) dentro de un nodo reusado. También es necesario si se quiere, en un futuro, crear **node presets** o compartir nodetrees.
- Notar: Si grupos funcionan, habría que exponer entradas/salidas de grupo, etc., lo cual Blender maneja con Group Input/Output nodes. Debemos registrar equivalentes para ComputeNodeTree (similar a Geometry Nodes). Esto quizá requiera definir ComputeNodeGroupInput y ComputeNodeGroupOutput que hereden de Node, poll en ComputeNodeTree, y se usan internamente. No es trivial pero vale la pena.
- Un posible riesgo es la compatibilidad con nuestra IR: al entrar a un grupo, hoy no existe concepto en IR, pero Blender "aplana" el group al evaluarlo (al menos en compositor, internamente conecta nodos). Dado que nuestro extraction recorre tree.nodes, esperemos que Blender expanda los grupos en node_tree.nodes accesibles directamente; si no, tendremos que recursivamente soportar nodetrees anidados. Esto es un tema para investigar; de no poder en esta versión, podríamos posponer grupos manteniendo todo en un nivel.
- **Backdrops o previews:** En el Compositor de Blender existe el _backdrop_ (fondo con la imagen resultante de un nodo Viewer). Sería muy útil para flujos de imágenes ver resultados intermedios sin salir del editor de nodos. Evaluaremos si es posible habilitar backdrop en nuestro editor:
- Blender activa backdrop si space_data.show_backdrop y con un node tipo Viewer. Podríamos introducir un ComputeNodeViewer que simplemente toma una imagen y la muestra, similar al Viewer del compositor. Sin embargo, en compositor el viewer dibuja en el UV/Image Editor; en Node Editor solo se ve como fondo. Quizá reutilizable.
- Si es demasiado esfuerzo, otra vía: facilitar al usuario enviar resultados a el editor de Imagen. Por ejemplo, permitiendo que el nodo Output cree o use un Image datablock, que el usuario puede abrir en UV/Image Editor. Esto ya sucede: Output tiene output_name y crea la imagen en bpy.data.images. Podemos mejorar la _usabilidad_ actual: en el panel lateral "Compute Runtime" podemos añadir un botón "Open Output in Image Editor" que abra la imagen de salida en una ventana de UV/Image. O incluso, automáticamente tras ejecutar, buscar si hay un área de UV Editor y refrescarla.
- Para previsualizar intermedios, podríamos sugerir al usuario usar múltiples Output nodos marcados como _Preview_ y tener varias imágenes (aunque esto no es interactivo como un backdrop). Alternativamente, implementar un nodo Viewer especial que al ejecutarse copie su input a Viewer Image (un datablock global) y luego actualice la ventana de image editor (similar a compositor). Este sería un _nice-to-have_ para la UX.
- **Controles de ejecución y feedback:** Ya se tiene un panel N con botón "Execute" y toggle auto_execute[\[50\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/operators.py#L120-L128). Es importante añadir quizás alguna **indicación de estado** cuando el grafo está ejecutándose, sobre todo si tardará. Blender no proporciona por defecto barra de progreso para compute shaders, pero podemos:
- Deshabilitar temporalmente el botón durante la ejecución (ya que es rápida normalmente, no crítico).
- Más útil: si se lanza una tarea muy pesada (imaginemos 8K imagen con 100 iteraciones), quizás convenga integrarse con el mecanismo de bpy.app.handlers para no bloquear la UI. Tal vez lanzar en un modal timer o en un thread (aunque Blender y threads es delicado). De momento, supondremos que la mayoría de operaciones son rápidos (ms a pocos sec). Para futuros, se puede explorar correr en un background thread y mostrar una animación de progreso en panel (difícil sin bloquear GL context, por ahora omitible).
- **Soporte de Undo/Redo:** Revisaremos que la edición en nuestro NodeTree entra al historial de _Undo_ de Blender correctamente (generalmente sí, ya que agregar/remover nodos, conectar sockets son ops registradas). Sin embargo, la ejecución del grafo no debe romper el deshacer (crear una imagen output modifica bpy.data, lo cual entra en undo stack - podríamos marcar la imagen creada como no-undo?). Por consistencia, probablemente convenga que crear la imagen de salida _sí_ sea registrable para undo (así si el usuario se arrepiente de haber ejecutado, borra la imagen).
- **Mensajes de error y depuración:** Cuando algo falla (p.ej. shader no compila, o el usuario conecta un tipo incorrecto que nuestro sistema no pudo inferir), debemos comunicarlo claramente:
- Usar node.error_message: Blender 4.0 introdujo la propiedad Node.message para mostrar errores en nodos. Podemos aprovechar esto: si en execute_compute_tree capturamos una excepción, podríamos encontrar qué nodo la causó (difícil si error es en shader compile). Al menos podríamos colocar un mensaje genérico en el panel o en un nodo especial (como Output).
- Un paso útil es loguear en la consola de Blender todos los _tracebacks_[\[51\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/operators.py#L76-L80) (ya se hace) pero eso no ayuda al usuario típico. Quizá en la UI, si ocurre fallo, podríamos desplegar un pop-up self.report({'ERROR'}, "...") que Blender muestra en la barra de estado.
- Para depuración avanzada (desarrollador), podríamos implementar en el panel un toggle "Debug" que, al ejecutar, imprima el GLSL generado o lo guarde en un texto Blender. De hecho, en execute_compute_tree ya se guarda p.display_source = p.source[\[52\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/operators.py#L38-L46), podríamos exponer eso. Tener el shader code visible ayuda a diagnosticar problemas de precisión, etc.
- **Documentación y ejemplos:** Para fomentar coherencia con expectativas de usuarios de nodos, añadiremos ejemplos de uso en la documentación (por ejemplo, "Cómo crear un blur progresivo usando loop ping-pong", "Cómo procesar partículas con ComputeNodes"), analogándolos con equivalentes en Geometry Nodes o Compositor para que ubiquen mentalmente el flujo. Idealmente, incluiremos esas demos dentro del addon (quizá un blend de ejemplo, o nodetrees prearmados).
- **Atajos y operabilidad:** Alinearemos atajos con Blender: por ejemplo _Mute node_ (M key) debería funcionar. Mute es soportado por Node.mute en Cycles/Compositor. En nuestros nodos, podemos respetarlo: Blender automáticamente no ejecuta nodos muteados y pasa su input directo a output. No obstante, habría que confirmar que ComputeNode.mute exista; si no, implementar un poll en extract_graph que ignore nodos con .mute. Esto facilita depurar un graph complejo (el usuario puede mutear temporalmente un nodo para ver efecto). Marcaremos los nodos muteados visualmente en interfaz (Blender lo hace).
- **Consistencia con Geometry Nodes en "Simulación":** Blender Geometry Nodes 4.0 introdujo _Simulation Zones_ (bucle para frame-by-frame). Aunque es un contexto distinto, los usuarios pueden esperar que nuestros Repeat Zones se comporten parecido. Podemos copiar ciertos UI cues: por ejemplo, Geometry Nodes dibuja un recuadro alrededor de nodos in/out de simulación. Podríamos estilar los Repeat Input/Output con un borde de color especial para indicar que forman pareja de loop (si Blender permite estilizar nodos, quizás con draw_buttons se puede jugar). No es crítico pero mejora UX visual.

En resumen, nuestra meta UX es que un artista familiarizado con nodos de Blender **no se sienta perdido** usando ComputeNodes. Debe reconocer patrones (menus, nombres, atajos) y encontrar algunas mejoras específicas de este contexto (p.ej. panel de ejecución). Al mismo tiempo, tenemos que exponer las nuevas capacidades (GLSL scripting, bucles complejos) de forma discoverable pero sin estorbar al flujo básico. Evaluaremos la necesidad de un manual dedicado o incorporar ayudas in-app (tooltips detallados, mensajes de consola que orienten si algo falta, etc.). Por ejemplo, si un usuario conecta una imagen de distinto tamaño y no inserta Resize, podríamos imprimir "Nota: reescalando automáticamente imagen X a tamaño Y". Esto da transparencia y educa al usuario de la característica.

## **Recomendaciones Técnicas y Plan de Implementación**

Para concluir, sintetizamos un **plan de acción** técnico para evolucionar ComputeNodes según lo analizado:

- **Refactorización Fundamental (Corto Plazo):**
- Integrar el sistema de **handlers modular** para la extracción IR, abarcando todos los nodos existentes y corrigiendo inconsistencias (especialmente Output, bucles, etc.). Eliminar el código legacy de graph_extract_old.py una vez comprobado que la versión modular reproduce los mismos resultados[\[21\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/graph_extract/registry.py#L10-L18)[\[26\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/graph_extract_old.py#L876-L885).
- Ajustar la **arquitectura IR** para soportar recursos con dimensión variable y tamaños independientes. Añadir campos o tipos según necesidad (ResourceDesc.dimensions, ResourceType.BUFFER, etc.). Asegurar que _schedule_passes_ no asume ya un solo tamaño global (preparar terreno para próximo paso).
- Reforzar el **TextureManager** y **ShaderManager** para manejar texturas 1D/3D: métodos como ensure_internal_texture(name, desc) deben contemplar dimensiones 1 o 3 (ya sea creando GPUTexture((w,), format=…) o GPUTexture((x,y,z), format=…) según caso). Probar estos con pequeños tests unitarios (simulando creación de 1D y 3D como en test_texture_manager_internal[\[53\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/tests/test_runtime.py#L94-L102)).
- **Kernels 1D/3D y Nuevos Tipos de Recurso (Mediano Plazo):**
- Implementar el **soporte 1D/3D** en la generación GLSL. Posiblemente esto implica variantes de ciertos emisores (p.ej., la operación IMAGE_LOAD deberá emitir imageLoad(img, coord_x) si es 1D, imageLoad(img, ivec3(x,y,z)) si 3D). Podemos codificar esto con chequeos en el emitter en base al tipo de ResourceDesc vinculado.
- Desarrollar nodos **Input/Output para Partículas** y **Volúmenes**. Esto incluye la lógica de conversión de datos Blender a recurso GPU y viceversa. Comenzar con casos sencillos: _Particles Input_ que solo lee posiciones como vector3 y los envía a GPU (dejando velocidades u otros atributos para después), _Volume Input_ que lee densidad de un grid estático. Verificar rendimiento con datasets pequeños antes de escalar.
- Añadir **opcodes IR** si faltan para manipular buffers (aunque quizá no se necesiten específicos más allá de READ/WRITE, ya existentes).
- **No exponer SSBO aún**: utilizar texturas 1D bajo el capó, pero diseñar la API de tal modo que, cuando se habilite GPUStorageBuffer, podamos intercambiar implementación sin cambiar la interfaz del nodo. (Por ejemplo, un socket "Buffer" seguirá funcionando, solo que en vez de textura usaría SSBO internamente).
- **Bucles y Flujo de Control Avanzado (Mediano Plazo):**
- Extender los nodos **Repeat Input/Output** para soportar múltiples variables. Internamente, reestructurar la construcción IR del loop (LOOP_START/END) para manejar listas de valores (podemos implementarlo primero con 1-2 variables para validar, luego generalizar N variables).
- Implementar **Ping-Pong**: quizá inicialmente como un nodo separado "PingPong Loop" para experimentar. Ensayar la técnica de iterar via múltiples dispatch en Python: e.g., en execute_compute_tree, detectar un PingPong node y realizar un bucle for i in range(n): dispatch shader, intercambiar bindings, dispatch shader,…. Evaluar sincronización (¿necesita gpu.compute.barrier()? Blender no expone barrera, pero cada dispatch es secuencial en Python).
- Implementar nodo **Resize** (y quizá _Rotate/Translate_) para tener herramientas de manipulación de resolución.
- Ajustar _schedule_passes_ para cortes por resolución. Testear con combinaciones: una rama half-res mezclada con full-res, etc., confirmando que efectivamente genera dos passes y que el shader del segundo puede acceder a la textura producida en el primero.
- Probar un bucle multi-res manual: por ejemplo, usar Repeat + dentro de loop un Resize a 50% -> procesar -> escalar de vuelta; asegurar que el scheduler organiza bien las passes.
- **Nodo de Scripting y Extensibilidad (Mediano - Largo Plazo):**
- Introducir **ComputeNodeScript** en Blender con un editor simple. Empezar soportando un caso básico (p.ej. una entrada float, una salida float, con código trivial) para montar la infraestructura: handler IR (que guarda el código en algún sitio), emitter GLSL (que inyecta el código).
- Desarrollar un mecanismo de **reemplazo de placeholders** en el código de usuario: decidir sintaxis (in0, out0 o usar nombres personalizados). Implementar un parser muy básico o sustitución de texto con precaución para no reemplazar adentro de palabras.
- Integrar la captura de errores de compilación: forzar un error deliberado en un script para ver qué arroja gpu.shader.create_from_info y cómo capturarlo (vía try/except ya presente[\[54\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/operators.py#L92-L100), complementándolo con el log).
- Añadir documentación contextual: un tooltip o modal en el UI del Script Node con instrucciones/resumen de variables disponibles, para guiar al usuario.
- Extender el **registry** para nodos custom de terceros: esto puede posponerse, pero se puede planear que tras estabilizar las partes anteriores, ofrezcamos funciones públicas para registro.
- **Mejoras de UX y pulido final (Largo Plazo):**
- Habilitar **node grouping** y verificar su funcionalidad. Si presenta trabas, coordinar con Blender dev (posiblemente ComputeNodeTree necesita marcar bl_idname = "ComputeNodeTree" con ciertas flags para que NodeGroup pueda usarse).
- Introducir **Viewer Node/backdrop**: implementar un ComputeNodeViewer que copie su entrada a un bpy.data.images\["Viewer Compute"\] cada vez que se ejecuta el tree. Configurar el Node Editor para usar backdrop con esa imagen (si técnicamente posible). Esta característica, aunque no esencial, acercaría la UX a la del Compositor, muy útil para depurar visualmente.
- Revisar **rendimiento UI**: si auto-exec está activado, que no recalcule excesivamente. Podríamos agregar pequeñas demoras (debounce) si detectamos ráfaga de cambios. Blender 4.x tiene un _flag_ NodeTree.changed y quizás un timer interno, pero si no, podríamos manualmente controlar.
- Garantizar que todas las interacciones (mute, delete, undo, reordenar nodos) funcionan correctamente sin errores. Si encontramos edge cases (ej. al eliminar un nodo Output mientras autoexec está on se lanza error porque faltan outputs - podríamos manejarlo con un chequeo en execute_compute_tree[\[55\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/operators.py#L86-L94) ya presente).
- **Testing con usuarios**: una vez todas las funcionalidades integradas, realizar pruebas comparativas: rehacer con ComputeNodes algún ejemplo hecho en Houdini Copernicus (obviamente dentro de lo que aplican: por ejemplo, un blur multires, una comp básica) para verificar que nuestra UX y resultado coinciden con expectativas. Ajustar detalles según feedback.

Por último, mantenernos al día con la evolución de Blender: Blender 5.x podría traer cambios en GPU API (transición completa a Vulkan, deprecación de GLSL). De hecho, en Blender 4.5 hay discusiones sobre GPUShaderCreateInfo y cómo manejar la creación de shaders multiplataforma[\[31\]](https://devtalk.blender.org/t/python-shadercreateinfo-missing-in-blender-4-5-what-s-the-plan/42007#:~:text=plan%3F%20devtalk,don%27t%20work%20under%20Vulkan). Nuestro diseño usando gpu.shader.create_from_info seguirá vigente, pero debemos testear en cada versión alpha. También, si Blender introduce oficialmente _Compute Nodes_ (nativo), habría que diferenciar nuestro addon o integrarlo. Hasta entonces, este plan posiciona a **ComputeNodes addon** como una herramienta potente, flexible y más alineada con estándares de la industria (Houdini/Nuke), a la vez que se integra de forma coherente con la experiencia Blender.

**Fuentes:** Este plan se fundamenta en el análisis del código actual del addon (definiciones de nodos, IR y ejecución)[\[41\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/graph_extract_old.py#L920-L928)[\[1\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/operators.py#L46-L55), en las prácticas comprobadas de sistemas nodales existentes (Houdini Copernicus[\[10\]](https://tokeru.com/cgwiki/HoudiniCops.html#:~:text=%2A%20New%20cops%20flow%20left,back%20to%20the%20viewport%20live)), y en capacidades confirmadas de la API de Blender a fecha de hoy[\[8\]](https://devtalk.blender.org/t/suggestions-feedback-on-the-extensions-for-the-gpu-module/17706/79?u=fclem#:~:text=Suggestions%20%2F%20feedback%20on%20the,sake%2C%20maybe%20something%20like). Cada recomendación técnica ha sido cuidadosamente evaluada para asegurar su viabilidad y beneficio dentro del contexto de Blender 5.0+. Con estas implementaciones, ComputeNodes evolucionará de un MVP experimental a un **sistema de producción** versátil comparable a soluciones profesionales, aprovechando al máximo las posibilidades de cómputo GPU en Blender.

[\[1\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/operators.py#L46-L55) [\[2\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/operators.py#L56-L64) [\[42\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/operators.py#L64-L71) [\[50\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/operators.py#L120-L128) [\[51\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/operators.py#L76-L80) [\[52\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/operators.py#L38-L46) [\[54\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/operators.py#L92-L100) [\[55\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/operators.py#L86-L94) operators.py

<https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/operators.py>

[\[3\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/graph_extract_old.py#L801-L810) [\[13\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/graph_extract_old.py#L838-L847) [\[15\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/graph_extract_old.py#L813-L822) [\[23\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/graph_extract_old.py#L17-L25) [\[24\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/graph_extract_old.py#L35-L44) [\[26\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/graph_extract_old.py#L876-L885) [\[27\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/graph_extract_old.py#L905-L914) [\[28\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/graph_extract_old.py#L8-L11) [\[32\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/graph_extract_old.py#L880-L889) [\[41\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/graph_extract_old.py#L920-L928) graph_extract_old.py

<https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/graph_extract_old.py>

[\[4\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/codegen/glsl.py#L88-L96) [\[19\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/codegen/glsl.py#L64-L72) [\[20\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/codegen/glsl.py#L56-L65) [\[22\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/codegen/glsl.py#L168-L176) [\[29\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/codegen/glsl.py#L148-L156) [\[38\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/codegen/glsl.py#L70-L73) glsl.py

<https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/codegen/glsl.py>

[\[5\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/tests/test_runtime.py#L140-L148) [\[6\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/tests/test_runtime.py#L150-L156) [\[30\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/tests/test_runtime.py#L142-L150) [\[39\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/tests/test_runtime.py#L85-L93) [\[40\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/tests/test_runtime.py#L117-L125) [\[43\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/tests/test_runtime.py#L80-L88) [\[44\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/tests/test_runtime.py#L94-L101) [\[53\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/tests/test_runtime.py#L94-L102) test_runtime.py

<https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/tests/test_runtime.py>

[\[7\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/planner/scheduler.py#L20-L28) scheduler.py

<https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/planner/scheduler.py>

[\[8\]](https://devtalk.blender.org/t/suggestions-feedback-on-the-extensions-for-the-gpu-module/17706/79?u=fclem#:~:text=Suggestions%20%2F%20feedback%20on%20the,sake%2C%20maybe%20something%20like) Suggestions / feedback on the extensions for the gpu module

<https://devtalk.blender.org/t/suggestions-feedback-on-the-extensions-for-the-gpu-module/17706/79?u=fclem>

[\[9\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/ir/ops.py#L100-L105) [\[16\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/ir/ops.py#L78-L86) [\[36\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/ir/ops.py#L66-L74) ops.py

<https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/ir/ops.py>

[\[10\]](https://tokeru.com/cgwiki/HoudiniCops.html#:~:text=%2A%20New%20cops%20flow%20left,back%20to%20the%20viewport%20live) Cops - Houdini and CG tips

<https://tokeru.com/cgwiki/HoudiniCops.html>

[\[11\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/nodes/control_flow.py#L114-L123) [\[12\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/nodes/control_flow.py#L132-L139) [\[14\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/nodes/control_flow.py#L115-L123) [\[33\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/nodes/control_flow.py#L30-L38) control_flow.py

<https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/nodes/control_flow.py>

[\[17\]](https://www.sidefx.com/docs/houdini/nodes/cop/index.html#:~:text=Wiring%20COPs%20together%20controls%20the,nodes%20that%20modify%20the%20data) [\[18\]](https://www.sidefx.com/docs/houdini/nodes/cop/index.html#:~:text=Image) [\[45\]](https://www.sidefx.com/docs/houdini/nodes/cop/index.html#:~:text=COP%20nodes%20provide%20real,manipulation%20within%20a%203D%20space) Copernicus nodes

<https://www.sidefx.com/docs/houdini/nodes/cop/index.html>

[\[21\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/graph_extract/registry.py#L10-L18) [\[25\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/graph_extract/registry.py#L11-L19) registry.py

<https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/graph_extract/registry.py>

[\[31\]](https://devtalk.blender.org/t/python-shadercreateinfo-missing-in-blender-4-5-what-s-the-plan/42007#:~:text=plan%3F%20devtalk,don%27t%20work%20under%20Vulkan) Python ShaderCreateInfo missing in Blender 4.5 what's the plan?

<https://devtalk.blender.org/t/python-shadercreateinfo-missing-in-blender-4-5-what-s-the-plan/42007>

[\[34\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/nodes/converter.py#L53-L61) [\[35\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/nodes/converter.py#L149-L157) converter.py

<https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/nodes/converter.py>

[\[37\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/ir/types.py#L4-L13) types.py

<https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/ir/types.py>

[\[46\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/nodes/input.py#L6-L14) input.py

<https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/nodes/input.py>

[\[47\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/nodes/output.py#L14-L22) output.py

<https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/nodes/output.py>

[\[48\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/nodes/math.py#L10-L19) math.py

<https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/nodes/math.py>

[\[49\]](https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/categories.py#L10-L19) categories.py

<https://github.com/pvtrcorps/ComputeNodes/blob/cb5b4d1b368e9e70e7ccbc466bd284ac0e0765f3/compute_nodes/categories.py>